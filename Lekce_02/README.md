# 2. lekce: Od VM k serverless

## Servery, kontejnery a bez-servery
Na tabuli si spole캜n캩 projdeme cestu z fyzick칳ch server콢 k virtu치ln칤m stroj콢m, kontejner콢 a nakonec serverless funkc칤m. P치r podklad콢 najdete tady:
- [Virtual Machine](https://en.wikipedia.org/wiki/Virtual_machine)
- [Comparing Containers and Virtual Machines](https://www.docker.com/resources/what-container/)
- [Functions as a Service](https://en.wikipedia.org/wiki/Function_as_a_service)

Automatizovat instalaci fyzick칳ch server콢 je n치ro캜n칠, ale mo쬹칠 (nap콏. Metal-as-a-Service od Canonical nebo Pliant od IBM). V cloudu je ale bare metal (fyzick칠 servery) slu쬭a, kter치 je dost exotick치 a b캩쬹캩 se nepou쮂셨치. V캩t코inou minimem jsou virtualizovan칠 stroje. Ty lze s vyu쬴t칤m cloudov칠ho API automatizovat velmi dob콏e. Uk치쬸e si p치r pokro캜ilej코칤ch cloud-native princip콢 pr치ce s VM. P콏esto쬰 jsou zm칤n캩n칠 techniky dob콏e funk캜n칤 a cloudov칳 poskytovatel pr치v캩 tohle pou쮂셨치 pro budov치n칤 vy코코칤ch slu쬰b nad t칤m (PaaS - nap콏칤klad datab치zi jako slu쬭a), firemn칤 klientela 캜asto provozuje VM "bare metal stylem", tedy stroj vytvo콏칤 a n캩kolik let ho opra코uj칤 (aktualizace apod.). Tento p콏칤stup je jist캩 v po콏치dku pro migraci tradi캜n칤ch aplikac칤 nep콏ipraven칳ch na cloud, ale pro modern캩j코칤 v캩ci bych doporu캜oval j칤t v칤ce cloud-native cestou (a mo쬹치 nez콢stat u VM viz d치le).

> **[Uk치zka: Green-Blue nasazen칤 VM](vm_green_blue.md) 游꿘 59:28**

```
# P콏칤kaz pro testov치n칤 loadbalancingu
while true; do cutl <IL loadbalanceru>; sleep 0.2; done
```

Cloud-native pou쬴t칤 VM je pom캩rn캩 t캩쬶op치dn칠. Image jsou velik칠, stroje pomalu startuj칤 a tak podobn캩. 콎e코en칤 postaven칠 na kontejnerech m치 mnoho z치sadn칤ch v칳hod:
- 맒치lovateln칠 a odleh캜en칠 (= rychl치 reakce na zm캩ny z치t캩쬰, plynul칠 a 캜ast칠 upgrady)
- Lep코칤 vyu쬴t칤 zdroj콢 (= ni쮄뫆 n치klady)
- Konzistence prost콏ed칤 - kontejner je jednotkou v칳po캜etn칤ho v칳konu i jednotkou nasazen칤
- Snadn캩j코칤 nasazen칤 (= rychlej코칤 v칳voj, m칠n캩 v칳padk콢)

Pro b캩h kontejner콢 typicky pou쬴jete orchestra캜n칤 n치stroj jako je Kubernetes (probereme v p콏칤코t칤 lekci detailn캩ji), ale 캜칤m d치l 캜ast캩ji nad n칤m vystavenou v칤ce abstrahovanou (jednodu코코칤 na pou쮂셨치n칤 a spolehliv칳 provoz) platformu (Azure Container Apps, AWS Fargate, Google Cloud Run). Tyto platformy v치m umo쬹칤 nasadit kontejnerovou aplikaci bez nutnosti spravovat orchestraci. Na krajn칤 캜치sti je pak serverless, kde u v콢bec nemus칤te 콏e코it infrastrukturu a plat칤te jen za to, co skute캜n캩 spot콏ebujete.

> **[Uk치zka: Kontejnery vs. Serverless](kontejnery_vs_serverless.md)**

## Auto코k치lov치n칤 a jeho aspekty
Auto코k치lov치n칤 je schopnost syst칠mu reagovat na zm캩ny z치t캩쬰 a p콏izp콢sobit se jim. M콢쬰 b칳t **vertik치ln칤** (p콏id치n칤 v칳konu na jednom stroji) nebo **horizont치ln칤** (p콏id치n칤 dal코칤ch stroj콢). Horizont치ln칤 코k치lov치n칤 je typick칠 pro cloud-native aplikace a je zalo쬰no na tom, 쬰 aplikace je schopn치 b캩쬰t na v칤ce instanc칤ch a je schopn치 si mezi sebou rozd캩lit pr치ci. U mnoha datab치zov칳ch syst칠m콢, kter칠 nab칤z칤 vysok칠 garance konzistence, ale horizont치ln칤 코k치lov치n칤 nen칤 k dispozici (cenou za n캩j je obvykle sn칤쬰n칤 garanc칤 konzistence - o t캩chto aspektech a NoSQL sv캩t캩 se budeme v kurzu bavit pozd캩ji).

캛asto m치 코k치lov치n칤 n캩kolik vrstev, nap콏칤klad:
1. Instance (kontejner) m치 rezervu, je schopen pojmout zv칳코enou z치t캩
2. V reakci na z치t캩 m콢쬰me nastartovat dal코칤 kontejnery
3. Pokud dojde m칤sto v clusteru, mus칤me nastartovat dal코칤 podkladov칠 nody
4. Pokud u nem콢쬰me nebo nechceme 코k치lovat v dan칠m regionu 캜i cloudu, m콢쬰me zv칳코it kapacitu jinde a pos칤lat n캩kter칠 u쬴vatele tam

Obvykle ty ni쮄뫆 vrstvy maj칤 velmi kr치tkou reak캜n칤 dobu, ty vy코코칤 v칳razn캩 v캩t코칤. Proto d치v치 smysl m칤t n캩jakou rezervu jej칤m c칤lem je ust치t to, ne nab캩hnou dal코칤 zdroje. N칤zk치 rezerva znamen치 ni쮄뫆 cenu, ale vy코코칤 riziko. Spustit 100 instanc칤 kdy pot콏ebuji jednu m치 nesm칤rn캩 mal칠 riziko, ale obrovskou cenu.

Rozhoduj칤c칤 je tak칠 architektura - asynchronn칤, ud치lostn캩 콏칤zen칠 콏e코en칤 je pro auto코k치lov치n칤 vhodn캩j코칤. Aplika캜n칤 architekturu budeme rozeb칤rat pozd캩ji v kurzu.

V neposledn칤 콏ad캩 n치s zaj칤m치 trigger, podle 캜eho pozn치me, 쬰 m치me 코k치lovat a jak to naladit. Krom캩 v칳po캜etn칤ch workload콢 nen칤 obvykle ide치ln칤 sledovat jednodu코e CPU, proto쬰 nemus칤 vypov칤dat o kvalit캩 slu쬭y pro u쬴vatele. Lep코칤 budou metriky jako je d칠lka fronty nebo odezvy.

Poznamenejme je코t캩, 쬰 obvykle se soust콏ed칤me na 코k치lov치n칤 nahoru (jsme 칰sp캩코n칤, chod칤 n치m lidi), co ovliv켿uje na코i top-line (prod치me v칤c). 맒치lov치n칤 dolu je m칠n캩 atraktivn칤, ale ovliv켿uje na코e n치klady. J칤t dolu ale m콢쬰 b칳t t캩쮄뫆 칰loha, zejm칠na u stavov칳ch komponent (nap콏. datab치ze).

## Paralelizace aneb jak si vyrobit superpo캜칤ta캜 (HPC)
Pod칤vejme se na dva probl칠my:
- P콏edpov칤d치n칤 po캜as칤 je v칳po캜etn캩 velmi n치ro캜n치 discipl칤na, kdy chceme v krajn칤m p콏칤pad캩 simulovat fyzik치ln칤 chov치n칤 ka쬯칠 molekuly vzduchu v atmosf칠콏e. Z praktick칳ch d콢vod콢 si sp칤코e rozd캩l칤me atmosf칠ru do n캩jak칳ch krychli캜ek. Ka쬯치 m치 na sv칳ch hran치ch a vn캩j코칤ch ploch치ch n캩jak칠 vlastnosti (nap콏. teplotu, tlak, vlhkost), kter칠 jsou v칳sledkem jej칤 interakce s okol칤m. My m콢쬰me prov칠st slo쬴t칳 v칳po캜et toho, co se bude n치sledn캩 uvnit콏 krychli캜ky d칤t a v칳sledkem bude n캩jak치 zm캩na na hran치ch a vn캩j코칤ch ploch치ch. Ka쬯ou kosti캜ku m콢쬰me po캜칤tat zvl치코콘, paraleln캩. Nicm칠n캩 - ona je i nad치le ovliv켿ov치na okoln칤mi a ty okoln칤 ovliv켿uje, tak쬰 nem치 cenu po캜칤tat to na n캩jak칠 p콏칤li코 dlouh칠 obdob칤. V칳sledky z jednotliv칳ch paraleln캩 b캩쮂셖칤ch 칰loh je pot콏eba si 캜asto vym캩켿ovat a synchronizovat. 
- Rendering 3D filmu je rovn캩 nesm칤rn캩 n치ro캜n치 칰loha na v칳po캜etn칤 kapacitu. Ka쬯칠 pol칤캜ko filmu je pot콏eba propo캜칤tat - zejm칠na co se t칳k치 drah sv캩tla, st칤n콢, odraz콢 a pr콢hlednosti. Ka쬯칠 pol칤캜ko je nez치visl칠 na ostatn칤ch, tak쬰 je mo쬹칠 je po캜칤tat paraleln캩. Doba v칳po캜tu z치le쮂 na slo쬴tosti sc칠ny, ale b칳v치 obvykle n캩kolik minut na siln칠 GPU, n캩kdy i n캩kolik hodin.

Jedna 칰loha je tightly coupled, druh치 embarassingly parallel. V prvn칤m p콏칤pad캩 je pot콏eba synchronizace a komunikace mezi jednotliv칳mi 캜치stmi, v druh칠m p콏칤pad캩 je mo쬹칠 jednotliv칠 캜치sti po캜칤tat nez치visle a v칳sledky n치sledn캩 jednodu코e spojit. Na hodin캩 proberme konsekvence z pohledu compute pot콏eb (zejm칠na s칤콘ov칠 propojen칤, latence, fyzick치 vzd치lenost uzl콢).

## Ekonomika cloudu z pohledu compute
Jednotkov칠 n치klady na vte콏inu b캩hu va코eho k칩du jsou rozhodn캩 nejni쮄뫆 v situaci, kdy se vezmete po콏치dn칳 kus (nap콏칤klad VM) a efektivita jeho zat칤쬰n칤 bude 캜ist캩 v치코 probl칠m. Pokud zvol칤te kontejnery jako slu쬭a, ztr치ty dan칠 provozem orchestra캜n칤 platformy, neobsazen칠 zdroje ve VM, neoptim치ln칤 rozlo쬰n칤 workloadu a tak podobn캩 jdou na bedra providera. U serverless je situace 칰pln캩 extr칠mn칤. V치코 k칩d je jednou za hodinu nap콏칤klad spust칤 v reakci na n캩jakou ud치lost a vy zaplat칤te pouhou vte콏inu jeho b캩hu. Overhead je obrovsk칳 a jde na bedra providera. Proto je VM v jednotkov칠 cen캩 nejlevn캩j코칤, ale va코e typick치 vyu쬴telnost jej칤ch zdroj콢 b칳v치 tak n칤zk치, 쬰 se v치m nevyplat칤.

| Typ slu쬭y | Typick치 utilizace | 1 hodinu |
|------------|-------------------|---------|
| VM v 3y rezervaci        | 10%               | 0,0155410959 EUR    |
| VM         | 10%               | 0,04088 EUR    |
| Kontejner  | 50%               | 0,097056 EUR     |
| Serverless | 95%               | 0,108 EUR   |


Z치kladn칤 ceny compute v cloudu jsou 칰캜tov치ny obvykle v 콏치du vte콏in 캜i minut a jde o tzv. Pay-as-you-go (PAYG). Pokud tedy zdroje nepou쮂셨치te, je dobr칠 je vypnout (= p콏estat platit), ide치ln캩 tedy pokud m칤sto jednoho velk칠ho zdroje pou쮂셨치te n캩kolik men코칤ch instanc칤 (typick칠 pro kontejnery) a jejich po캜et p콏izp콢sobujete z치t캩쬴. Pokud ale n캩jak칳 compute pot콏ebujete dlouhodob캩, provider v치m za to d치 n캩jakou formou slevu:
- Rezervace zdroje v z치vazku 1 nebo 3 roky s r콢znou m칤rou flexibility. Na horn칤 hranici flexibility jsou t콏eba Savings Plans (Azure, AWS), kde se zavazujete k spot콏eb캩 vyj치d콏en칠 v pen캩z칤ch na vyjmenovan칠 compute slu쬭y libovoln칳ch SKU i region콢. Na druh칠m konci spektra jsou Reserverd Instance v krajn칤m p콏칤pad캩 uzam캜en칠 na konkr칠tn칤 SKU a region s platbou dop콏edu a tam je sleva nejv캩t코칤.
- U Google jsou Committed Use Contracts (podobn칠 v칳코e zm칤n캩n칳m metod치m) a tak칠 Sustained Use Discount (defacto sn칤쬰n칤 ceny pro stroj, kter칳 v dan칠m m캩s칤ci u b캩쬰l dlouho).

**P콏칤klad 1** 
M치te Kubernetes cluster na ka쬯칠m kontinentu, kter칳 코k치luje podle toho, zda m칤stn칤 z치kazn칤ci sp칤 nebo ne a to n치sleduj칤c칤m zp콢sobem:
- V noci 8 core
- Od 9 r치no do 5 ve캜er v pracovn칤ch dnech 16 core
- Zpracov치n칤 dat p콏es v칤kend - 16 core na asi 4 hodiny v pr콢b캩hu v칤kendu
- Nep콏edv칤dateln칠 코pi캜ky jsou obvykle v d칠lce 1 hodina a je pot콏eba 32 core

Jak bychom tohle mohli nakoupit?
- Neflexibiln칤 rezervovan칠 instance v po캜tu 3x 8 core v t캩ch regionech (sleva nap콏. 65%)
- Flexibiln칤 콏e코en칤 typu Savings Plans na nap콏칤klad na 8 core, proto쬰 v칤cem칠n캩 ka쬯ou hodinu n캩kde b캩쮂 (sleva nap콏. 30%)
- Zbytek je 코patn캩 p콏edv칤dateln칳 a je lep코칤 ho pokr칳t z PAYG bez slevy

**P콏칤klad 2**
M치te farmu virtu치ln칤ch desktop콢 pro zam캩stnance a tady jsou pot콏eby z hlediska v칳konu:
- V쬯y (ve dne i v noci) 8 core
- 20h v pracovn칤ch dnech a 10h o v칤kendu 32 core
- 6h v pracovn칤 dny 48 core
- 1h v pracovn칤 dny v nejv캩t코칤 코pi캜ce 64 core

Na co si koupit rezervovanou instanci bude z치le쬰t na slev캩 (po캜칤tejme pro zjednodu코en칤 m캩s칤c co m치 28 dn칤):
- 8 core si koup칤me ur캜it캩, pot콏ebujeme trvale
- Dal코칤ch 24 core pot콏ebujeme 20*20+10*8 = 480h m캩s칤캜n캩 z 672h, tedy p콏i slev캩 alespo켿 28,6% a to ur캜it캩 dostaneme, tak쬰 jdeme do rezervace (a nemus칤me stroje v콢bec vyp칤nat)
- Dal코칤ch 16 core pot콏ebujeme 6*20 = 120h m캩s칤캜n캩 z 672h, tedy sleva by musela b칳t 82% - to u pravd캩podobn캩 bude v칤c, ne provider nab칤z칤
- Dal코칤ch 16 core na 코pi캜ky pot콏ebujeme jen 20h m캩s칤캜n캩, tam ur캜it캩 bude mnohem levn캩j코칤 jet v PAYG, zap칤nat stroje jen podle pot콏eby

